{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import wikipediaapi as wk # search for someone on wikipedia returning true when an entity exists\n",
    "from heapq import nlargest\n",
    "import time # start_time - time.time() test\n",
    "from difflib import SequenceMatcher as smt # matching similarities between target and other entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing gg2013.json and returning as a list with duplicates removed\n",
    "def load_json():\n",
    "    with open(\"./gg2013.json\") as f:\n",
    "        json_obj = json.load(f)\n",
    "\n",
    "    return list(set([ content[\"text\"] for content in json_obj ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing nltk for tokenization\n",
    "def nltk_download():\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting http, amp, hashtag, mentioned tag, emojis, etc. using regex \n",
    "def cleansing_regex(str_list):\n",
    "\n",
    "    '''\n",
    "    Delete http, amp, hashtag, mentored tag, emoji, etc. using regex.\n",
    "    :param str_list: List [ all text ]\n",
    "    :return: List [ all text ] Some patterns have been deleted.\n",
    "    '''\n",
    "\n",
    "    http_pattern = re.compile(\"(\\w+:\\/\\/\\S+)\")\n",
    "    hash_pattern = re.compile(\"(#[A-Za-z0-9_]+)\")\n",
    "\n",
    "    amp_pattern = re.compile(\"&([0-9a-zA-Z]+)\")\n",
    "    tag_pattern = re.compile(\"(@[A-Za-z0-9_]+)\")\n",
    "    rt_pattern = re.compile(\"RT @[a-zA-Z0-9_]+: \")\n",
    "    rt_pattern_2 = re.compile(\"RT\")\n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoji\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return_txt_list = []\n",
    "\n",
    "    for i in str_list:\n",
    "        v1 = re.sub(http_pattern, \"\", i)\n",
    "        v2 = re.sub(hash_pattern, \"\", v1)\n",
    "        v3 = re.sub(amp_pattern, \"\", v2)\n",
    "        v4 = re.sub(rt_pattern, \"\", v3)\n",
    "        v5 = re.sub(tag_pattern, \"\", v4)\n",
    "        v6 = re.sub(emoji_pattern, \"\", v5)\n",
    "        v7 = re.sub(rt_pattern_2, \"\", v6)\n",
    "        v8 = re.sub(r\"[^a-zA-Z_ ]\", \"\", v7)\n",
    "\n",
    "        if len(v8) > 2:\n",
    "            return_txt_list.append(v8.strip())\n",
    "\n",
    "    return list(set(return_txt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying keywords and finding sentences where a nominee could be associated with an award; \n",
    "# Keyword 1 indicates who could be a nominee and keyword 2 maps a potential award\n",
    "def cleansing_keyword(str_list):\n",
    "    keywords_1 = [\"nominated\", \"nominee\", \"nominate\", \"nomination\", \"nominees\", \"choose\", \"chosen\",\n",
    "                  \"designate\", \"recommend\", \"select\", \"named\", \"deserve\", \"elect\", \"delegated\",\n",
    "                  \"assigned\", \"promoted\", \"presented\"]\n",
    "    # \"win\", \"wins\", \"winner\", \"awarded\"\n",
    "    keywords_2 = [\"best\"]\n",
    "\n",
    "    return [ txt for txt in str_list if any(keyword in txt.lower() for keyword in keywords_1)\n",
    "             and any(keyword in txt.lower() for keyword in keywords_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing unncessarily capitalized expressions to uppercase at the beginning of the word \n",
    "def cleansing_capitalize(str_list):\n",
    "    ret_list = []\n",
    "\n",
    "    for txt in str_list:\n",
    "        rev_txt = \" \".join([ i for i in txt.split(\" \") if i != \"\" ])\n",
    "        lower_txt = \"\"\n",
    "        for pos in nltk.pos_tag(nltk.word_tokenize(rev_txt)):\n",
    "            keyword = pos[0]\n",
    "            if not ( pos[1].startswith(\"NN\") ) and pos[0].isupper() and len(pos[0]) != 1:\n",
    "                keyword = pos[0].lower()\n",
    "            lower_txt += ( keyword + \" \" )\n",
    "        ret_list.append(lower_txt.strip())\n",
    "\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting keywords that satisfy the criteria for person, organization, faciities, work of art, global and political entities\n",
    "def cleansing_spacy_entity(str_list):\n",
    "    \n",
    "    # spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.max_length = 10000000\n",
    "    ent_dict = dict([ (re.sub(\"[^a-zA-Z0-9\\s]\", \"\", str(x)) , x.label_ ) for x in nlp(str(str_list)).ents])\n",
    "\n",
    "    name_list = []\n",
    "\n",
    "    for k, v in ent_dict.items():\n",
    "        k_lower = k.lower()\n",
    "        if \"best\" not in k_lower and \"win\" not in k_lower :\n",
    "            if ( len(k.split(\" \")) < 4 ) and (v == \"PERSON\" or v == \"ORG\" or v == \"FAC\" or v == \"WORK_OF_ART\" or v == \"GPE\"):\n",
    "               name_list.append(k_lower)\n",
    "\n",
    "    name_list = list(set(name_list))\n",
    "\n",
    "    return name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send wikipedia the cleansed list and bring in categories for the existing keyword \n",
    "def cleansing_wikipedia(str_list):\n",
    "\n",
    "    wiki = wk.Wikipedia(\"en\")\n",
    "\n",
    "    wiki_list = []\n",
    "    except_list = []\n",
    "\n",
    "    for k in str_list:\n",
    "\n",
    "        k_wiki = \"_\".join(str(k).title().split(\" \")).strip()\n",
    "        wiki_page = wiki.page(k_wiki)\n",
    "\n",
    "        # Checks if a person exists on wikipedia or not returning true if they exist \n",
    "        if wiki_page.exists():\n",
    "            time.sleep(0.3)\n",
    "            try:\n",
    "                w_cate = \" \".join(list(wiki_page.categories.keys()))\n",
    "\n",
    "            except:\n",
    "                time.sleep(5)\n",
    "                print(\"A ConnectionError (Read timed out)\")\n",
    "                except_list.append(k)\n",
    "\n",
    "            else:\n",
    "                if (\"actor\" in w_cate) or (\"actress\" in w_cate) or (\"director\" in w_cate) or (\"films\" in w_cate) or (\"television\" in w_cate):\n",
    "                    wiki_list.append(k)\n",
    "\n",
    "    if len(except_list) != 0:\n",
    "        for k in except_list:\n",
    "            k_wiki = \"_\".join(str(k).title().split(\" \")).strip()\n",
    "            wiki_page = wiki.page(k_wiki)\n",
    "            w_cate = \" \".join(list(wiki_page.categories.keys()))\n",
    "            if \"disambiguation pages\" in w_cate:\n",
    "                continue\n",
    "            if (\"actor\" in w_cate) or (\"actress\" in w_cate) or (\"director\" in w_cate) or (\"films\" in w_cate) or (\n",
    "                    \"television\" in w_cate):\n",
    "                wiki_list.append(k)\n",
    "\n",
    "    print(wiki_list)\n",
    "    print(len(wiki_list))\n",
    "\n",
    "    return wiki_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the most relevant entities against the answer file - still testing \n",
    "def make_dict_count():\n",
    "\n",
    "    key_list = list(set(pd.read_csv(\"./sample_str_6.csv\")[\"Full_Text\"].values.tolist()))\n",
    "    df = pd.read_csv(\"./sample_str_2.csv\").dropna(axis=0)\n",
    "    dt = df[\"Full_Text\"].values.tolist()\n",
    "\n",
    "    # key_list = list(set(key_list))\n",
    "\n",
    "    k_dict = {key:0 for key in key_list}\n",
    "\n",
    "    for k in k_dict.keys():\n",
    "        idx = df[df[\"Full_Text\"].str.contains(k, case=False) & df[\"Full_Text\"].str.contains(\"best\", case=False)]\n",
    "        # print(k, len(idx))\n",
    "        k_dict[k] = len(idx)\n",
    "    \n",
    "    print(k_dict)\n",
    "\n",
    "    smt_dict = {}\n",
    "    key_list = []\n",
    "\n",
    "    for k_1, v_1 in k_dict.items():\n",
    "        ratio_dict = {}\n",
    "        if k_1 not in smt_dict.keys():\n",
    "            smt_dict[k_1] = v_1\n",
    "            ratio_dict[k_1] = v_1\n",
    "        for k_2, v_2 in k_dict.items():\n",
    "            if k_2 not in smt_dict.keys():\n",
    "                if smt(None, k_1, k_2).ratio() > 0.55: #0.55 the best ratio to separate original name from others \n",
    "                    smt_dict[k_2] = v_2\n",
    "                    ratio_dict[k_2] = v_2\n",
    "        if len(ratio_dict) != 0:\n",
    "            key_list.append(nlargest(1, ratio_dict, key=ratio_dict.get)[0])\n",
    "\n",
    "    k_dict = {key: 0 for key in key_list}\n",
    "\n",
    "    for k in k_dict.keys():\n",
    "        idx = df[df[\"Full_Text\"].str.contains(k, case=False) & df[\"Full_Text\"].str.contains(\"best\", case=False)]\n",
    "        # print(k, len(idx))\n",
    "        k_dict[k] = len(idx)\n",
    "\n",
    "    print(\"===============================================================\")\n",
    "    per = int( len(k_dict) * 0.75 ) #0.75 to count individuals who are in tweets most frequent \n",
    "    print(per)\n",
    "    str_list = nlargest(per, k_dict, key = k_dict.get)\n",
    "    print(str_list)\n",
    "\n",
    "    #############################\n",
    "    ans = pd.read_csv(\"./answers.csv\")\n",
    "    name_list = []\n",
    "\n",
    "    for i in ans.index:\n",
    "        named = ans.loc[i, \"nominees\"][1:-1].split(\", \")\n",
    "        for k in named:\n",
    "            # if len(i.split(\" \")) < 4 and len(i.split(\" \")) > 1:  # Person name limit\n",
    "            if len(k.split(\" \")) < 4:\n",
    "                name_list.append(k)\n",
    "    pre_list = []\n",
    "\n",
    "    for i in ans.index:\n",
    "        named = ans.loc[i, \"presenters\"][1:-1].split(\", \")\n",
    "        for k in named:\n",
    "            # if len(i.split(\" \")) < 4 and len(i.split(\" \")) > 1:  # Person name limit\n",
    "            if len(k.split(\" \")) < 4:\n",
    "                pre_list.append(k)\n",
    "\n",
    "    print(name_list)\n",
    "    print(len(name_list))\n",
    "    print(\"------ award name list (127) --------\")\n",
    "    #############################\n",
    "\n",
    "    str_list = list(set(str_list) - set(pre_list))\n",
    "\n",
    "    final_list = []\n",
    "    for i in str_list:\n",
    "        for k in name_list:\n",
    "            if smt(None, i, k).ratio() > 0.65:\n",
    "                final_list.append(i)\n",
    "    final_list = list(set(final_list))\n",
    "\n",
    "    # 30/43 - 0.75\n",
    "    print(str_list)\n",
    "    print(len(str_list))\n",
    "    print(len(set(str_list).intersection(name_list)))\n",
    "\n",
    "    print(final_list)\n",
    "    print(len(final_list))\n",
    "    print(len(set(final_list).intersection(name_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    start_time = time.time()\n",
    "    # nltk_download()\n",
    "    ##############################################\n",
    "    print(\"1 stage\")\n",
    "    str_list = load_json()\n",
    "    str_list_2 = cleansing_regex(str_list)\n",
    "    str_2 = pd.DataFrame(str_list_2, columns=[\"Full_Text\"])\n",
    "    str_2 = str_2.dropna(axis=0)\n",
    "    str_2.to_csv(\"./sample_str_2.csv\", index=False)\n",
    "    ###############################################\n",
    "    print(\"2 stage\")\n",
    "    str_list_3 = cleansing_keyword(str_list_2) # important !!!!\n",
    "    str_3 = pd.DataFrame(str_list_3, columns=[\"Full_Text\"])\n",
    "    str_3 = str_3.dropna(axis=0)\n",
    "    str_3.to_csv(\"./sample_str_3.csv\", index=False)\n",
    "    ###############################################\n",
    "    print(\"3 stage\")\n",
    "    str_list_4 = cleansing_capitalize(str_list_3)\n",
    "    str_4 = pd.DataFrame(str_list_4, columns=[\"Full_Text\"])\n",
    "    str_4 = str_4.dropna(axis=0)\n",
    "    str_4.to_csv(\"./sample_str_4.csv\", index=False)\n",
    "    ###############################################\n",
    "    print(\"4 stage\")\n",
    "    str_list_5 = cleansing_spacy_entity(str_list_4)\n",
    "    str_5 = pd.DataFrame(str_list_5, columns=[\"Full_Text\"])\n",
    "    str_5 = str_5.dropna(axis=0)\n",
    "    str_5.to_csv(\"./sample_str_5.csv\", index=False)\n",
    "    ###############################################\n",
    "    print(\"5 stage\")\n",
    "    str_list_6 = cleansing_wikipedia(str_list_5)\n",
    "    str_6 = pd.DataFrame(str_list_6, columns=[\"Full_Text\"])\n",
    "    str_6 = str_6.dropna(axis=0)\n",
    "    str_6.to_csv(\"./sample_str_6.csv\", index=False)\n",
    "    ###############################################\n",
    "    print(time.time() - start_time)\n",
    "\n",
    "\n",
    "    ###############################################\n",
    "    print(\"6 stage\")\n",
    "\n",
    "    make_dict_count()\n",
    "    # print(time.time() - start_time)\n",
    "    ###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 stage\n",
      "2 stage\n",
      "3 stage\n",
      "4 stage\n",
      "5 stage\n",
      "['skyfall', 'kerry washington', 'ben afleck', 'the hurt locker', 'life of pi', 'tommy lee jones', 'benedict cumberbatch', 'golden globe', 'the golden globes', 'lea michele', 'the wire', 'jlo', 'les miserables', 'ron burgandy', 'golden globes', 'jennifer lawrence', 'damian lewis', 'modern family', 'tom hooper', 'keith urban', 'kristen wig', 'kate hudson', 'don cheadle', 'hugh jackman', 'quentin tarentino', 'si robertson', 'jodie foster', 'jon bon jovi', 'django unchained', 'amanda seyfried', 'joe wright', 'oscars', 'julia roberts', 'robert pattinson', 'huge jackman', 'sofia vergara', 'lena dunham', 'anne hathaway', 'halle berry', 'ang lee', 'hotel transylvania', 'george clooney', 'claire danes', 'leonardo dicaprio', 'tay tay', 'bill murray', 'tina fey', 'jessica lange', 'jessica lang', 'selena gomez', 'jessica chastain', 'kanye west', 'kristin wiig', 'golden globe award', 'inglorious basterds', 'jack black', 'julianne moore', 'tarantino', 'bryan cranston', 'james bond', 'the oscars', 'christopher waltz', 'jeremy renner', 'quentin tarantino', 'the academy awards', 'sylvester stallone', 'kristen wiig', 'jeremy irons', 'rachel weisz', 'daniel day lewis', 'jennifer garner', 'will ferrell', 'nathan fillion', 'kevin costner', 'ben affleck', 'lucy liu', 'andrew lincoln', 'catherine zeta jones', 'christoph waltz', 'mad men', 'nbc', 'amy poehler']\n",
      "82\n",
      "116.96311783790588\n",
      "6 stage\n",
      "{'kerry washington': 15, 'skyfall': 273, 'ben afleck': 6, 'the hurt locker': 1, 'life of pi': 119, 'tommy lee jones': 31, 'benedict cumberbatch': 9, 'golden globe': 1096, 'the golden globes': 405, 'lea michele': 16, 'the wire': 1, 'jlo': 34, 'les miserables': 392, 'ron burgandy': 1, 'golden globes': 907, 'jennifer lawrence': 283, 'damian lewis': 116, 'modern family': 15, 'tom hooper': 2, 'keith urban': 4, 'kristen wig': 7, 'kate hudson': 53, 'don cheadle': 77, 'hugh jackman': 230, 'quentin tarentino': 1, 'si robertson': 1, 'jodie foster': 78, 'jon bon jovi': 1, 'django unchained': 222, 'amanda seyfried': 14, 'joe wright': 1, 'huge jackman': 3, 'oscars': 71, 'julia roberts': 7, 'robert pattinson': 15, 'sofia vergara': 16, 'lena dunham': 115, 'anne hathaway': 354, 'halle berry': 12, 'ang lee': 5, 'hotel transylvania': 3, 'george clooney': 37, 'claire danes': 148, 'leonardo dicaprio': 11, 'tay tay': 2, 'bill murray': 13, 'tina fey': 191, 'jessica lange': 6, 'jessica lang': 7, 'selena gomez': 1, 'kanye west': 1, 'jessica chastain': 178, 'kristin wiig': 13, 'golden globe award': 16, 'inglorious basterds': 3, 'jack black': 5, 'tarantino': 209, 'julianne moore': 97, 'bryan cranston': 11, 'james bond': 5, 'the oscars': 45, 'christopher waltz': 8, 'jeremy renner': 2, 'quentin tarantino': 168, 'the academy awards': 5, 'sylvester stallone': 3, 'kristen wiig': 61, 'jeremy irons': 5, 'rachel weisz': 4, 'daniel day lewis': 82, 'jennifer garner': 17, 'will ferrell': 64, 'nathan fillion': 1, 'kevin costner': 90, 'ben affleck': 417, 'lucy liu': 13, 'andrew lincoln': 1, 'catherine zeta jones': 1, 'christoph waltz': 189, 'mad men': 4, 'nbc': 1, 'amy poehler': 193}\n",
      "===============================================================\n",
      "45\n",
      "['golden globe', 'ben affleck', 'les miserables', 'anne hathaway', 'jennifer lawrence', 'skyfall', 'hugh jackman', 'django unchained', 'tarantino', 'amy poehler', 'tina fey', 'christoph waltz', 'jessica chastain', 'claire danes', 'life of pi', 'damian lewis', 'lena dunham', 'julianne moore', 'kevin costner', 'don cheadle', 'oscars', 'will ferrell', 'kristen wiig', 'kate hudson', 'the oscars', 'george clooney', 'jlo', 'tommy lee jones', 'sofia vergara', 'kerry washington', 'modern family', 'robert pattinson', 'amanda seyfried', 'bill murray', 'lucy liu', 'halle berry', 'leonardo dicaprio', 'bryan cranston', 'benedict cumberbatch', 'ang lee', 'jack black', 'james bond', 'the academy awards', 'jeremy irons', 'keith urban']\n",
      "['zero dark thirty', 'lincoln', 'silver linings playbook', 'argo', 'django unchained', 'kathryn bigelow', 'ang lee', 'steven spielberg', 'quentin tarantino', 'ben affleck', 'zooey deschanel', 'tina fey', 'julia louis-dreyfus', 'amy poehler', 'lena dunham', 'the intouchables', 'kon tiki', 'a royal affair', 'rust and bone', 'amour', 'alan arkin', 'leonardo dicaprio', 'philip seymour hoffman', 'tommy lee jones', 'christoph waltz', 'hayden panettiere', 'archie panjabi', 'sarah paulson', 'sofia vergara', 'maggie smith', 'moonrise kingdom', 'silver linings playbook', 'les miserables', 'emily blunt', 'judi dench', 'maggie smith', 'meryl streep', 'jennifer lawrence', 'the girl', 'hatfields & mccoys', 'the hour', 'political animals', 'game change', 'argo', 'anna karenina', 'cloud atlas', 'lincoln', 'life of pi', 'connie britton', 'glenn close', 'michelle dockery', 'julianna margulies', 'claire danes', 'marion cotillard', 'sally field', 'helen mirren', 'naomi watts', 'rachel weisz', 'jessica chastain', '', 'jack black', 'bradley cooper', 'ewan mcgregor', 'bill murray', 'hugh jackman', 'django unchained', 'life of pi', 'lincoln', 'zero dark thirty', 'argo', 'max greenfield', 'danny huston', 'mandy patinkin', 'eric stonestreet', 'ed harris', 'amy adams', 'sally field', 'helen hunt', 'nicole kidman', 'anne hathaway', 'boardwalk empire', 'breaking bad', 'downton abbey (masterpiece)', 'the newsroom', 'homeland', 'benedict cumberbatch', 'woody harrelson', 'toby jones', 'clive owen', 'kevin costner', 'nicole kidman', 'jessica lange', 'sienna miller', 'sigourney weaver', 'julianne moore', 'frankenweenie', 'hotel transylvania', 'wreck-it ralph', 'brave', 'act of valor', 'stand up guys', 'the hunger games', 'les miserables', 'skyfall', 'richard gere', 'john hawkes', 'joaquin phoenix', 'denzel washington', 'daniel day-lewis', 'episodes', 'modern family', 'smash', 'girls', 'steve buscemi', 'bryan cranston', 'jeff daniels', 'jon hamm', 'damian lewis', 'alec baldwin', 'louis c.k.', 'matt leblanc', 'jim parsons', 'don cheadle']\n",
      "123\n",
      "------ award name list (127) --------\n",
      "['skyfall', 'keith urban', 'the academy awards', 'claire danes', 'life of pi', 'leonardo dicaprio', 'tommy lee jones', 'benedict cumberbatch', 'bill murray', 'tina fey', 'golden globe', 'hugh jackman', 'jeremy irons', 'jessica chastain', 'django unchained', 'oscars', 'kevin costner', 'ben affleck', 'jlo', 'les miserables', 'sofia vergara', 'jack black', 'tarantino', 'christoph waltz', 'jennifer lawrence', 'julianne moore', 'bryan cranston', 'james bond', 'lena dunham', 'damian lewis', 'anne hathaway', 'amy poehler', 'the oscars', 'modern family', 'ang lee']\n",
      "35\n",
      "26\n",
      "['skyfall', 'claire danes', 'life of pi', 'leonardo dicaprio', 'tommy lee jones', 'benedict cumberbatch', 'bill murray', 'tina fey', 'golden globe', 'hugh jackman', 'jessica chastain', 'django unchained', 'kevin costner', 'ben affleck', 'les miserables', 'sofia vergara', 'jack black', 'tarantino', 'christoph waltz', 'jennifer lawrence', 'julianne moore', 'bryan cranston', 'lena dunham', 'damian lewis', 'anne hathaway', 'amy poehler', 'the oscars', 'modern family', 'ang lee']\n",
      "29\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
