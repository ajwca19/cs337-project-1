{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# import wikipedia as wk\n",
    "import time\n",
    "# from pywikipedia import wikipedia as pw\n",
    "import wikipediaapi as wk\n",
    "# https://pypi.org/project/Wikipedia-API/\n",
    "\n",
    "\n",
    "def load_json():\n",
    "\n",
    "    with open(\"./gg2013.json\") as f:\n",
    "        json_obj = json.load(f)\n",
    "\n",
    "    return list(set([ content[\"text\"] for content in json_obj ]))\n",
    "\n",
    "def cleansing_1(str_list):\n",
    "\n",
    "    http_pattern = re.compile(\"(\\w+:\\/\\/\\S+)\")\n",
    "    hash_pattern = re.compile(\"(#[A-Za-z0-9_]+)\")\n",
    "    amp_pattern = re.compile(\"&([0-9a-zA-Z]+)\")\n",
    "    tag_pattern = re.compile(\"(@[A-Za-z0-9_]+)\")\n",
    "    rt_pattern = re.compile(\"RT @[a-zA-Z0-9_]+: \")\n",
    "    rt_pattern_2 = re.compile(\"RT\")\n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return_txt_list = []\n",
    "\n",
    "    for i in str_list:\n",
    "\n",
    "        v1 = re.sub(http_pattern, \"\", i)\n",
    "        # v2 = re.sub(hash_pattern, \"\", v1)\n",
    "        v3 = re.sub(amp_pattern, \"\", v1)\n",
    "        v4 = re.sub(rt_pattern, \"\", v3)\n",
    "        v5 = re.sub(tag_pattern, \"\", v4)\n",
    "        v6 = re.sub(emoji_pattern, \"\", v5)\n",
    "        v7 = re.sub(rt_pattern_2, \"\", v6)\n",
    "        v8 = re.sub(r\"[^a-zA-Z_ ]\", \"\", v7)\n",
    "\n",
    "        if len(v8) > 2:\n",
    "\n",
    "            return_txt_list.append(v8.strip())\n",
    "\n",
    "    return return_txt_list\n",
    "\n",
    "def cleansing_2(str_list):\n",
    "\n",
    "    keywords_1 = [\"nominated\", \"nominee\", \"nominate\", \"nomination\", \"nominees\"]\n",
    "    # keywords_2 = [\"best\", \"win\", \"wins\"]\n",
    "    # return [txt for txt in str_list if any(keyword in txt.lower() for keyword in keywords_1) and any(keyword in txt.lower() for keyword in keywords_2)]\n",
    "    return [txt for txt in str_list if any(keyword in txt.lower() for keyword in keywords_1) ]\n",
    "    # return [ txt for txt in str_list if any(keyword in txt.lower() for keyword in keywords_1) ]\n",
    "\n",
    "def nltk_download():\n",
    "\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')  # Lemmatization\n",
    "    nltk.download('omw-1.4')  # Lemmatization\n",
    "    # nltk.download('maxent_ne_chunker')\n",
    "    # nltk.download('words')\n",
    "\n",
    "    print(\"NLTK Download END\")\n",
    "\n",
    "def nltk_filter(str_list):\n",
    "\n",
    "    wlem = nltk.WordNetLemmatizer()\n",
    "    nltk_list = []\n",
    "\n",
    "    with open(\"basic_stopword.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        stopword = f.read()\n",
    "    stopword_list = stopword.split()\n",
    "\n",
    "    '''\n",
    "    pattern = re.compile(r\"[^\\w'.,:;]|_\")\n",
    "\n",
    "    for index, value in enumerate(str_list):\n",
    "        str_list[index] = \" \".join(re.sub(pattern, \" \", value).strip().split())\n",
    "    '''\n",
    "\n",
    "    for index, value in enumerate(str_list):\n",
    "\n",
    "        # print(index)\n",
    "\n",
    "        topic_list = []\n",
    "\n",
    "        for pos in nltk.pos_tag(nltk.word_tokenize(value)):\n",
    "             # if ( pos[0]. startswith(\"best\") ):\n",
    "             #   print(pos)\n",
    "             # if (pos[1].startswith(\"JJ\") or pos[1].startswith(\"NN\") or pos[1].startswith(\"RB\") or pos[1].startswith(\n",
    "             #        \"VB\") ) \\\n",
    "             #        and (\"'\" not in pos[0]):\n",
    "                ori_keyword = pos[0].strip()\n",
    "                pre_keyword = pos[0].lower().strip()  # pos_tagging -> keyword ( Wayne )\n",
    "\n",
    "                if pre_keyword in stopword_list:\n",
    "                    continue\n",
    "                if len(pre_keyword) < 3:\n",
    "                    continue\n",
    "\n",
    "                keyword = wlem.lemmatize(pre_keyword)  # keyword -> lemmatize -> keyword ( Wa )\n",
    "\n",
    "\n",
    "                if len(pre_keyword) < 3:\n",
    "                    continue\n",
    "                if keyword not in stopword_list:\n",
    "                    # topic_list.append(keyword)\n",
    "                    if pos[1].startswith(\"NN\"):\n",
    "                        topic_list.append(ori_keyword)\n",
    "                    else :\n",
    "                        topic_list.append(ori_keyword.lower())\n",
    "\n",
    "        nltk_list.append(\" \".join(topic_list))\n",
    "\n",
    "    return nltk_list\n",
    "\n",
    "def cleansing_3(str_list):\n",
    "\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    person_list = []\n",
    "\n",
    "    for txt in str_list:\n",
    "        # print(txt)\n",
    "        for entity in nlp(txt).ents:\n",
    "            # if entity.label_ == \"PERSON\" or entity.label_ == \"ORG\" or entity.label_ == \"PRODUCT\" or entity.label_ == \"WORD_OF_ART\":\n",
    "            if entity.label_ == \"PERSON\":\n",
    "                ent_str_lower = str(entity).lower()\n",
    "                if \"win\" not in ent_str_lower and \"best\" not in ent_str_lower:\n",
    "                    ent_str = re.sub(r\"[^a-zA-Z0-9_ ]\", \"\", str(entity))\n",
    "                    person_list.append([txt, ent_str.strip(), entity.label_])\n",
    "\n",
    "\n",
    "    return person_list\n",
    "\n",
    "def load_str_4():\n",
    "\n",
    "    df = pd.read_csv(\"./sample_str_4.csv\")\n",
    "    ans = pd.read_csv(\"./answers.csv\")\n",
    "\n",
    "    #############################\n",
    "    name_list = []\n",
    "\n",
    "    for i in ans.index:\n",
    "        named = ans.loc[i, \"nominees\"][1:-1].split(\", \")\n",
    "        for i in named:\n",
    "            if len(i.split(\" \")) < 3: # Person name limit\n",
    "                name_list.append(i)\n",
    "\n",
    "    print(name_list)\n",
    "    print(len(name_list))\n",
    "    print(\"------ award name list (127) --------\")\n",
    "    #############################\n",
    "\n",
    "    unique_dict = {}\n",
    "\n",
    "    for i in df.index:\n",
    "        dict_key = df.loc[i, \"Entity\"].lower()\n",
    "\n",
    "        if len(dict_key.split(\" \")) < 3: # Person Name Limit\n",
    "\n",
    "            if dict_key not in unique_dict.keys():\n",
    "                unique_dict[dict_key] = 1\n",
    "            elif dict_key in unique_dict.keys():\n",
    "                unique_dict[dict_key] += 1\n",
    "    print(\"len(unique_dict) : \", len(unique_dict))\n",
    "    print(\"len(unique_dict).intersection : \", len(set(list(unique_dict)).intersection(set(name_list))))\n",
    "\n",
    "    unique_dict = {key: value for key, value in unique_dict.items() if value > 1}\n",
    "\n",
    "    # unique_dict = {key: value for key, value in unique_dict.items() if value > 1}\n",
    "\n",
    "    wiki_dict = {}\n",
    "    wkk = wk.Wikipedia('en')\n",
    "\n",
    "    for k, v in unique_dict.items():\n",
    "        # print(k, v)\n",
    "        k_split = str(k).title().split(\" \")\n",
    "        if len(k_split) != 1:\n",
    "            k_wiki = \"_\".join(str(k).title().split(\" \")).strip()\n",
    "        else :\n",
    "            continue\n",
    "        # print(k_wiki.strip())\n",
    "        # k_wiki = str(k).title()\n",
    "        # print(k_wiki)\n",
    "        # if wk.suggest(k_wiki) != None:\n",
    "        #    wiki_dict[k] = v\n",
    "        # try:\n",
    "        #     print(wk.page(k_wiki))\n",
    "        # except:\n",
    "        #     print(\"그런 위키는 없어요\")\n",
    "        # else:\n",
    "        #     wiki_dict[k] = v\n",
    "\n",
    "        if wkk.page(k_wiki).exists() == True:\n",
    "            # print(k_wiki)\n",
    "            wiki_dict[k] = v\n",
    "\n",
    "    print(\"len(wiki_dict) : \", len(wiki_dict))\n",
    "    print(\"len(wiki_dict).intersection : \", len(set(list(wiki_dict)).intersection(set(name_list))))\n",
    "    # Success\n",
    "    # unique_dict = {key:value for key, value in unique_dict.items() if value > 1}\n",
    "\n",
    "    print(\"len(unique_dict) : \", len(unique_dict))\n",
    "    print(\"len(unique_dict).intersection : \", len(set(list(unique_dict)).intersection(set(name_list))))\n",
    "\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "    '''\n",
    "    unique_dict_2 = {}\n",
    "\n",
    "    for k_1, v_1 in unique_dict.items():\n",
    "        alive = k_1\n",
    "        num = v_1\n",
    "        for k_2, v_2 in unique_dict.items():\n",
    "            if k_1 in k_2 or k_2 in k_1:\n",
    "                if ( k_1 != k_2 ):\n",
    "                    if v_1 > v_2:\n",
    "                        alive = k_1\n",
    "                        num = v_1\n",
    "                    elif v_2 > v_1:\n",
    "                        alive = k_2\n",
    "                        num= v_2\n",
    "\n",
    "        if alive not in unique_dict_2.keys():\n",
    "            # unique_dict_2[alive] = num\n",
    "            exist = False\n",
    "            for k, v in unique_dict_2.items():\n",
    "\n",
    "                if ( str(alive) in str(k) ) or ( str(k) in str(alive) ):\n",
    "\n",
    "                    exist = True\n",
    "\n",
    "                    if v != 0:\n",
    "                        if num > v:\n",
    "                            exist = False\n",
    "                            # unique_dict_2[alive] = num\n",
    "                            unique_dict_2[k] = 0\n",
    "\n",
    "                        elif v > num :\n",
    "                            continue\n",
    "            if exist == False:\n",
    "                unique_dict_2[alive] = num\n",
    "\n",
    "    # unique_dict_2 = {key:value for key, value in unique_dict_2.items() if value != 0}\n",
    "\n",
    "    print(\"unique_dict_2\")\n",
    "    print(\"len(unique_dict_2) : \", len(unique_dict_2))\n",
    "    print(\"len(set(list(unique_dict_2)).intersection : \", len(set(list(unique_dict_2)).intersection(set(name_list))))\n",
    "    \n",
    "    \n",
    "    filter_dict = {key.lower(): value for key, value in unique_dict_2.items() if value != 0 }\n",
    "\n",
    "    print(\"filter_dict\")\n",
    "    print(\"len(filter_dict) : \", len(filter_dict))\n",
    "    print(\"len(set(list(filter_dict)).intersection : \", len(set(list(filter_dict)).intersection(set(name_list))))\n",
    "    '''\n",
    "\n",
    "    ################################\n",
    "\n",
    "\n",
    "\n",
    "    # 1 - 63 all - 72 / 127\n",
    "    # print(len(set(list(filter_dict)).intersection(set(name_list))))\n",
    "    # print(len(set(list(unique_dict)).intersection(set(name_list))))\n",
    "    # print(len(set(list(unique_dict_2)).intersection(set(name_list))))\n",
    "    print(\"END\")\n",
    "    # if wk.suggest(\"sdfsdfsdfsdfsdf\") == None :\n",
    "    #    print(\"hello\")\n",
    "    wiki_dict\n",
    "def main():\n",
    "\n",
    "\n",
    "    str_list = load_json()\n",
    "    str_list_2 = cleansing_1(str_list)\n",
    "\n",
    "    str_2 = pd.DataFrame(str_list_2, columns=[\"Full_Text\"])\n",
    "    str_2 = str_2.dropna(axis=0)\n",
    "    str_2.to_csv(\"./sample_str_2.csv\", index=False)\n",
    "\n",
    "    str_list_3 = cleansing_2(str_list_2)\n",
    "\n",
    "    str_3 = pd.DataFrame(str_list_3, columns=[\"Full_Text\"])\n",
    "    str_3 = str_3.dropna(axis=0)\n",
    "    str_3.to_csv(\"./sample_str_3.csv\", index=False)\n",
    "\n",
    "    nltk_download()\n",
    "    nltk_list = nltk_filter(str_list_3)\n",
    "\n",
    "    nltk_df = pd.DataFrame(nltk_list, columns=[\"Full_Text\"])\n",
    "    nltk_df = nltk_df.dropna(axis=0)\n",
    "    nltk_df.to_csv(\"./nltk_list.csv\", index=False)\n",
    "\n",
    "    str_list_4 = cleansing_3(nltk_list)\n",
    "\n",
    "    str_4 = pd.DataFrame(str_list_4, columns=[\"Full_Text\", \"Entity\", \"Entity_Label\"])\n",
    "    str_4 = str_4.dropna(axis=0)\n",
    "    str_4 = str_4.astype(\"str\")\n",
    "    str_4 = str_4.sort_values(by=[\"Full_Text\"], ascending=True)\n",
    "    str_4.to_csv(\"./sample_str_4.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    load_str_4()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
